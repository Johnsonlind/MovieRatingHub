# ==========================================
# ä¸»ç¨‹åº
# ==========================================
import asyncio
import os
from dotenv import load_dotenv
import time
import ssl
from typing import Optional
import hashlib
import httpx
import logging

load_dotenv()

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

from fastapi import FastAPI, HTTPException, Request, Depends, APIRouter, Response
from fastapi.security import OAuth2PasswordBearer, OAuth2
from fastapi.middleware.cors import CORSMiddleware
from jose import JWTError, jwt
from passlib.context import CryptContext
from datetime import datetime, timedelta
from models import SQLALCHEMY_DATABASE_URL, User, Favorite, FavoriteList, SessionLocal, PasswordReset, Follow, ChartEntry, PublicChartEntry, SchedulerStatus
from sqlalchemy.orm import Session, selectinload
from ratings import extract_rating_info, get_tmdb_info, RATING_STATUS, search_platform, create_rating_data
from redis import asyncio as aioredis
import json
import base64
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import smtplib
import secrets
import aiohttp
from fastapi.security.utils import get_authorization_scheme_param
from fastapi.openapi.models import OAuthFlows as OAuthFlowsModel
from sqlalchemy import func, or_, not_, and_, create_engine
from sqlalchemy.pool import QueuePool
from fastapi.middleware.gzip import GZipMiddleware
from browser_pool import browser_pool
import traceback

# ==========================================
# 1. é…ç½®å’Œåˆå§‹åŒ–éƒ¨åˆ†
# ==========================================

REDIS_URL = "redis://:l1994z0912x@localhost:6379/0"
CACHE_EXPIRE_TIME = 24 * 60 * 60
CHARTS_CACHE_EXPIRE = 2 * 60
redis = None

SECRET_KEY = "L1994z0912x."
ALGORITHM = "HS256"
ACCESS_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7
REMEMBER_ME_TOKEN_EXPIRE_DAYS = 30

FRONTEND_URL = "http://localhost:5173" if os.getenv("ENV") == "development" else "https://ratefuse.cn"

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:5173",
        "http://ratefuse.cn",
        "https://ratefuse.cn",
        "http://www.ratefuse.cn",
        "https://www.ratefuse.cn"
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.add_middleware(GZipMiddleware, minimum_size=1000)

engine = create_engine(
    SQLALCHEMY_DATABASE_URL,
    poolclass=QueuePool,
    pool_size=10,
    max_overflow=20,
    pool_timeout=30,
    pool_recycle=1800
)

# ==========================================
# 2. è¾…åŠ©ç±»å’Œå‡½æ•°
# ==========================================

class OAuth2PasswordBearerOptional(OAuth2):
    def __init__(
        self,
        tokenUrl: str,
        scheme_name: Optional[str] = None,
        scopes: Optional[dict] = None,
        auto_error: bool = True,
    ):
        if not scopes:
            scopes = {}
        flows = OAuthFlowsModel(password={"tokenUrl": tokenUrl, "scopes": scopes})
        super().__init__(flows=flows, scheme_name=scheme_name, auto_error=auto_error)

    async def __call__(self, request: Request) -> Optional[str]:
        authorization: str = request.headers.get("Authorization")
        if not authorization:
            return None
            
        scheme, param = get_authorization_scheme_param(authorization)
        if not authorization or scheme.lower() != "bearer":
            return None
            
        return param

pwd_context = CryptContext(schemes=["bcrypt"], deprecated="auto", bcrypt__rounds=10)
oauth2_scheme = OAuth2PasswordBearer(tokenUrl="token")
oauth2_scheme_optional = OAuth2PasswordBearerOptional(tokenUrl="token", auto_error=False)

def get_db():
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def verify_password(plain_password, hashed_password):
    return pwd_context.verify(plain_password, hashed_password)

def get_password_hash(password):
    return pwd_context.hash(password)

def create_access_token(data: dict, remember_me: bool = False):
    to_encode = data.copy()
    if remember_me:
        expire = datetime.utcnow() + timedelta(days=REMEMBER_ME_TOKEN_EXPIRE_DAYS)
    else:
        expire = datetime.utcnow() + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)
    return encoded_jwt

async def get_current_user(
    token: str = Depends(oauth2_scheme),
    db: Session = Depends(get_db)
):
    credentials_exception = HTTPException(
        status_code=401,
        detail="æ— æ•ˆçš„è®¤è¯å‡­æ®",
        headers={"WWW-Authenticate": "Bearer"},
    )
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            raise credentials_exception
    except JWTError:
        raise credentials_exception
        
    user = db.query(User).filter(User.email == email).first()
    if user is None:
        raise credentials_exception
    return user

async def get_current_user_optional(
    token: Optional[str] = Depends(oauth2_scheme_optional),
    db: Session = Depends(get_db)
) -> Optional[User]:
    if not token:
        return None
    
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        email: str = payload.get("sub")
        if email is None:
            return None
            
        user = db.query(User).filter(User.email == email).first()
        if user is None:
            return None
            
        return user
    except JWTError:
        return None

async def get_cache(key: str):
    """ä» Redis è·å–ç¼“å­˜æ•°æ®"""
    if not redis:
        return None
        
    try:
        data = await redis.get(key)
        
        if data:
            data = json.loads(data)
            if isinstance(data, dict) and "status" in data:
                if data.get("status") == RATING_STATUS["SUCCESSFUL"]:
                    return data
                return None
            return data
        return None
    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜å‡ºé”™: {e}")
        return None

async def set_cache(key: str, data: dict, expire: int = CACHE_EXPIRE_TIME):
    """å°†æ•°æ®å­˜å…¥ Redis ç¼“å­˜"""
    if not redis:
        return
    try:
        if isinstance(data, dict) and "status" in data:
            if data.get("status") == RATING_STATUS["SUCCESSFUL"]:
                await redis.setex(key, expire, json.dumps(data))
        else:
            await redis.setex(key, expire, json.dumps(data))
    except Exception as e:
        logger.error(f"è®¾ç½®ç¼“å­˜å‡ºé”™: {e}")


async def get_tmdb_info_cached(id: str, type: str, request: Request):
    """å¸¦ Redis ç¼“å­˜çš„ TMDB åŸºç¡€ä¿¡æ¯è·å–"""
    cache_key = f"tmdb:info:{type}:{id}"
    cached = await get_cache(cache_key)
    if cached:
        return cached
    tmdb_info = await get_tmdb_info(id, type, request)
    if tmdb_info:
        await set_cache(cache_key, tmdb_info, expire=CACHE_EXPIRE_TIME)
    return tmdb_info

def generate_reset_token():
    return secrets.token_urlsafe(32)

def check_following_status(db: Session, follower_id: Optional[int], following_id: int) -> bool:
    if not follower_id:
        return False
    
    follow = db.query(Follow).filter(
        Follow.follower_id == follower_id,
        Follow.following_id == following_id
    ).first()
    
    return bool(follow)

# ==========================================
# 3. ç”¨æˆ·è®¤è¯ç›¸å…³è·¯ç”±
# ==========================================

@app.post("/auth/register")
async def register(
    request: Request,
    db: Session = Depends(get_db)
):
    data = await request.json()
    email = data.get("email")
    username = data.get("username")
    password = data.get("password")
    
    if db.query(User).filter(User.email == email).first():
        raise HTTPException(
            status_code=400,
            detail="è¯¥é‚®ç®±å·²è¢«æ³¨å†Œ"
        )
    
    if db.query(User).filter(User.username == username).first():
        raise HTTPException(
            status_code=400,
            detail="è¯¥ç”¨æˆ·åå·²è¢«ä½¿ç”¨"
        )
    
    hashed_password = get_password_hash(password)
    user = User(
        email=email,
        username=username,
        hashed_password=hashed_password
    )
    
    db.add(user)
    db.commit()
    db.refresh(user)
    
    access_token = create_access_token(
        data={"sub": user.email},
        remember_me=False
    )
    
    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": {
            "id": user.id,
            "email": user.email,
            "username": user.username
        }
    }

@app.post("/auth/login")
async def login(request: Request, db: Session = Depends(get_db)):
    try:
        data = await request.json()
        email = data.get("email")
        password = data.get("password")
        remember_me = data.get("remember_me", False)
        
        user = db.query(User).filter(User.email == email).first()
        
        if not user:
            raise HTTPException(
                status_code=401,
                detail="æ­¤é‚®ç®±æœªæ³¨å†Œ"
            )
        
        if not verify_password(password, user.hashed_password):
            raise HTTPException(
                status_code=401,
                detail="é‚®ç®±æˆ–å¯†ç é”™è¯¯"
            )
        
        access_token = create_access_token(
            data={"sub": user.email},
            remember_me=remember_me
        )
        
        return {
            "access_token": access_token,
            "token_type": "bearer",
            "user": {
                "id": user.id,
                "email": user.email,
                "username": user.username,
                "avatar": user.avatar,
                "is_admin": getattr(user, "is_admin", False),
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ç™»å½•è¿‡ç¨‹å‡ºé”™: {str(e)}")
        logger.error(traceback.format_exc())
        raise HTTPException(
            status_code=500,
            detail=f"ç™»å½•å¤±è´¥: {str(e)}"
        )

@app.post("/auth/forgot-password")
async def forgot_password(
    request: Request,
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        email = data.get("email")
        
        user = db.query(User).filter(User.email == email).first()
        if not user:
            raise HTTPException(
                status_code=404,
                detail="æœªæ‰¾åˆ°è¯¥é‚®ç®±å¯¹åº”çš„ç”¨æˆ·"
            )
        
        token = generate_reset_token()
        expires_at = datetime.utcnow() + timedelta(hours=24)
        
        reset_record = PasswordReset(
            email=email,
            token=token,
            expires_at=expires_at
        )
        db.add(reset_record)
        db.commit()
        
        reset_link = f"{FRONTEND_URL}/reset-password?token={token}"
        sender_email = "ratefuseteam@gmail.com"
        app_password = "ukhtkvzexnzgeibw"
        
        message = MIMEMultipart()
        message["From"] = f"RateFuse <{sender_email}>"
        message["To"] = email
        message["Subject"] = "RateFuse - é‡ç½®å¯†ç "
        
        body = f"""
        <html>
          <body>
            <h2>é‡ç½®æ‚¨çš„ RateFuse å¯†ç </h2>
            <p>æ‚¨å¥½ï¼</p>
            <p>æˆ‘ä»¬æ”¶åˆ°äº†é‡ç½®æ‚¨ RateFuse è´¦æˆ·å¯†ç çš„è¯·æ±‚ã€‚è¯·ç‚¹å‡»ä¸‹é¢çš„é“¾æ¥é‡ç½®å¯†ç ï¼š</p>
            <p><a href="{reset_link}">é‡ç½®å¯†ç </a></p>
            <p>å¦‚æœæ‚¨æ²¡æœ‰è¯·æ±‚é‡ç½®å¯†ç ï¼Œè¯·å¿½ç•¥æ­¤é‚®ä»¶ã€‚</p>
            <p>æ­¤é“¾æ¥å°†åœ¨ 24 å°æ—¶åå¤±æ•ˆã€‚</p>
            <br>
            <p>RateFuse å›¢é˜Ÿ</p>
          </body>
        </html>
        """
        
        message.attach(MIMEText(body, "html"))
        
        max_retries = 3
        retry_count = 0

        while retry_count < max_retries:
            try:
                context = ssl.create_default_context()
                with smtplib.SMTP_SSL("smtp.gmail.com", 465, context=context) as server:
                    server.login(sender_email, app_password)
                    text = message.as_string()
                    server.sendmail(sender_email, email, text)
                    return {"message": "é‡ç½®å¯†ç é‚®ä»¶å·²å‘é€"}
            except Exception as e:
                retry_count += 1
                if retry_count == max_retries:
                    raise HTTPException(
                        status_code=500,
                        detail=f"å‘é€é‚®ä»¶å¤±è´¥: {str(e)}"
                    )
                time.sleep(2)
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"å¤„ç†è¯·æ±‚å¤±è´¥: {str(e)}"
        )

@app.post("/auth/reset-password")
async def reset_password(
    request: Request,
    db: Session = Depends(get_db)
):
    data = await request.json()
    token = data.get("token")
    new_password = data.get("password")
    
    reset_record = db.query(PasswordReset).filter(
        PasswordReset.token == token,
        PasswordReset.used == False,
        PasswordReset.expires_at > datetime.utcnow()
    ).first()
    
    if not reset_record:
        raise HTTPException(
            status_code=400,
            detail="æ— æ•ˆæˆ–å·²è¿‡æœŸçš„é‡ç½®é“¾æ¥"
        )
    
    user = db.query(User).filter(User.email == reset_record.email).first()
    if not user:
        raise HTTPException(
            status_code=404,
            detail="æœªæ‰¾åˆ°ç”¨æˆ·"
        )
    
    user.hashed_password = get_password_hash(new_password)
    reset_record.used = True
    db.commit()
    
    return {"message": "å¯†ç é‡ç½®æˆåŠŸ"}

@app.get("/user/me")
async def read_user_me(current_user: User = Depends(get_current_user)):
    return {
        "id": current_user.id,
        "email": current_user.email,
        "username": current_user.username
    }

@app.get("/api/user/me")
async def get_current_user_info(
    current_user: User = Depends(get_current_user)
):
    return {
        "id": current_user.id,
        "email": current_user.email,
        "username": current_user.username,
        "avatar": current_user.avatar,
        "is_admin": current_user.is_admin
    }

@app.put("/api/user/douban-cookie")
async def update_douban_cookie(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ›´æ–°ç”¨æˆ·çš„è±†ç“£Cookie"""
    try:
        data = await request.json()
        cookie = data.get("cookie", "").strip()
        
        if cookie:
            current_user.douban_cookie = cookie
        else:
            current_user.douban_cookie = None
        
        db.commit()
        
        return {
            "message": "è±†ç“£Cookieæ›´æ–°æˆåŠŸ" if cookie else "è±†ç“£Cookieå·²æ¸…é™¤",
            "has_cookie": bool(cookie)
        }
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°è±†ç“£Cookieæ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/user/douban-cookie")
async def get_douban_cookie(
    current_user: User = Depends(get_current_user)
):
    """è·å–ç”¨æˆ·çš„è±†ç“£CookieçŠ¶æ€"""
    return {
        "has_cookie": bool(current_user.douban_cookie)
    }

@app.put("/api/user/profile")
async def update_profile(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        
        if data.get("avatar"):
            if not data["avatar"].startswith('data:image/'):
                raise HTTPException(
                    status_code=400,
                    detail="æ— æ•ˆçš„å›¾ç‰‡æ ¼å¼"
                )
            avatar_data = data["avatar"].split(',')[1]
            if len(avatar_data) > 2 * 1024 * 1024:
                raise HTTPException(
                    status_code=400,
                    detail="å›¾ç‰‡å¤§å°ä¸èƒ½è¶…è¿‡ 2MB"
                )
            current_user.avatar = data["avatar"]
        
        if data.get("username"):
            existing_user = db.query(User).filter(
                User.username == data["username"],
                User.id != current_user.id
            ).first()
            if existing_user:
                raise HTTPException(
                    status_code=400,
                    detail="è¯¥ç”¨æˆ·åå·²è¢«ä½¿ç”¨"
                )
            current_user.username = data["username"]
        
        if data.get("password"):
            current_user.hashed_password = get_password_hash(data["password"])
        
        db.commit()
        
        return {
            "message": "ä¸ªäººèµ„æ–™æ›´æ–°æˆåŠŸ",
            "user": {
                "id": current_user.id,
                "email": current_user.email,
                "username": current_user.username,
                "avatar": current_user.avatar
            }
        }
    except Exception as e:
        db.rollback()
        logger.error(f"æ›´æ–°ä¸ªäººèµ„æ–™æ—¶å‡ºé”™: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

# ==========================================
# 4. æ”¶è—ç›¸å…³è·¯ç”±
# ==========================================

@app.post("/api/favorites")
async def add_favorite(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        
        required_fields = ["media_id", "media_type", "title", "poster", "list_id"]
        for field in required_fields:
            if field not in data:
                raise HTTPException(
                    status_code=400,
                    detail=f"ç¼ºå°‘å¿…è¦å­—æ®µ: {field}"
                )
        
        favorite_list = db.query(FavoriteList).filter(
            FavoriteList.id == data["list_id"],
            FavoriteList.user_id == current_user.id
        ).first()
        
        if not favorite_list:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—åˆ—è¡¨ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®"
            )
        
        max_sort_order = db.query(func.max(Favorite.sort_order)).filter(
            Favorite.list_id == data["list_id"]
        ).scalar()
        
        favorite = Favorite(
            user_id=current_user.id,
            list_id=data["list_id"],
            media_id=data["media_id"],
            media_type=data["media_type"],
            title=data["title"],
            poster=data["poster"],
            year=data.get("year", ""),
            note=data.get("note"),
            overview=data.get("overview", ""),
            sort_order=(max_sort_order + 1) if max_sort_order is not None else 0
        )
        
        db.add(favorite)
        db.commit()
        db.refresh(favorite)
        
        return {
            "message": "æ”¶è—æˆåŠŸ",
            "favorite": {
                "id": favorite.id,
                "media_id": favorite.media_id,
                "media_type": favorite.media_type,
                "title": favorite.title,
                "poster": favorite.poster,
                "year": favorite.year,
                "note": favorite.note,
                "overview": favorite.overview
            }
        }
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"æ·»åŠ æ”¶è—å¤±è´¥: {str(e)}"
        )

@app.get("/api/favorites")
async def get_favorites(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        favorites = db.query(Favorite).filter(
            Favorite.user_id == current_user.id
        ).all()
        
        return [{
            "id": fav.id,
            "media_id": fav.media_id,
            "media_type": fav.media_type,
            "title": fav.title,
            "poster": fav.poster,
            "year": fav.year,
            "overview": fav.overview,
            "note": fav.note
        } for fav in favorites]
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–æ”¶è—å¤±è´¥: {str(e)}"
        )

@app.delete("/api/favorites/{favorite_id}")
async def delete_favorite(
    favorite_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        favorite = db.query(Favorite).filter(
            Favorite.id == favorite_id,
            Favorite.user_id == current_user.id
        ).first()
        
        if not favorite:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—ä¸å­˜åœ¨æˆ–æ— æƒé™åˆ é™¤"
            )
        
        db.delete(favorite)
        db.commit()
        
        return {"message": "æ”¶è—åˆ é™¤æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤æ”¶è—å¤±è´¥: {str(e)}"
        )

@app.put("/api/favorites/{favorite_id}")
async def update_favorite(
    favorite_id: int,
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        
        favorite = db.query(Favorite).filter(
            Favorite.id == favorite_id,
            Favorite.user_id == current_user.id
        ).first()
        
        if not favorite:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—ä¸å­˜åœ¨æˆ–æ— æƒé™ä¿®æ”¹"
            )
        
        if "note" in data:
            favorite.note = data["note"]
        
        db.commit()
        db.refresh(favorite)
        
        return {
            "id": favorite.id,
            "media_id": favorite.media_id,
            "media_type": favorite.media_type,
            "title": favorite.title,
            "poster": favorite.poster,
            "year": favorite.year,
            "overview": favorite.overview,
            "note": favorite.note
        }
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"æ›´æ–°æ”¶è—å¤±è´¥: {str(e)}"
        )

@app.get("/api/favorite-lists")
async def get_favorite_lists(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        lists = (
            db.query(FavoriteList)
            .options(selectinload(FavoriteList.favorites))
            .filter(FavoriteList.user_id == current_user.id)
            .all()
        )

        original_list_ids = {lst.original_list_id for lst in lists if lst.original_list_id}
        original_lists_map = {}
        original_creators_map = {}

        if original_list_ids:
            original_lists = (
                db.query(FavoriteList)
                .options(selectinload(FavoriteList.user))
                .filter(FavoriteList.id.in_(original_list_ids))
                .all()
            )
            for ol in original_lists:
                original_lists_map[ol.id] = ol
                if ol.user:
                    original_creators_map[ol.id] = {
                        "id": ol.user.id,
                        "username": ol.user.username,
                        "avatar": ol.user.avatar,
                    }

        creator = {
            "id": current_user.id,
            "username": current_user.username,
            "avatar": current_user.avatar,
        }

        result = []
        for lst in lists:
            original_creator = original_creators_map.get(lst.original_list_id)

            favorites = sorted(
                lst.favorites or [],
                key=lambda fav: (
                    fav.sort_order is None,
                    fav.sort_order if fav.sort_order is not None else 0,
                    fav.id,
                ),
            )

            result.append(
                {
                    "id": lst.id,
                    "name": lst.name,
                    "description": lst.description,
                    "is_public": lst.is_public,
                    "created_at": lst.created_at,
                    "original_list_id": lst.original_list_id,
                    "original_creator": original_creator,
                    "creator": creator,
                    "favorites": [
                        {
                            "id": fav.id,
                            "media_id": fav.media_id,
                            "media_type": fav.media_type,
                            "title": fav.title,
                            "poster": fav.poster,
                            "year": fav.year,
                            "overview": fav.overview,
                            "note": fav.note,
                            "sort_order": fav.sort_order,
                        }
                        for fav in favorites
                    ],
                }
            )

        return result
    except Exception as e:
        logger.error(f"è·å–æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}")

@app.post("/api/favorite-lists")
async def create_favorite_list(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        
        if not data.get("name"):
            raise HTTPException(
                status_code=400,
                detail="åˆ—è¡¨åç§°ä¸èƒ½ä¸ºç©º"
            )
        
        existing_list = db.query(FavoriteList).filter(
            FavoriteList.user_id == current_user.id,
            FavoriteList.name == data["name"]
        ).first()
        
        if existing_list:
            raise HTTPException(
                status_code=400,
                detail="å·²å­˜åœ¨åŒåæ”¶è—åˆ—è¡¨"
            )
        
        new_list = FavoriteList(
            user_id=current_user.id,
            name=data["name"],
            description=data.get("description"),
            is_public=data.get("is_public", False)
        )
        
        db.add(new_list)
        db.commit()
        db.refresh(new_list)
        
        return {
            "id": new_list.id,
            "name": new_list.name,
            "description": new_list.description,
            "is_public": new_list.is_public,
            "created_at": new_list.created_at,
            "favorites": []
        }
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"åˆ›å»ºæ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.get("/api/favorite-lists/{list_id}")
async def get_favorite_list(
    list_id: int,
    current_user: Optional[User] = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    list_data = db.query(FavoriteList).filter(FavoriteList.id == list_id).first()
    if not list_data:
        raise HTTPException(status_code=404, detail="åˆ—è¡¨ä¸å­˜åœ¨")

    response_data = {
        "id": list_data.id,
        "name": list_data.name,
        "description": list_data.description,
        "is_public": list_data.is_public,
        "user_id": list_data.user_id,
        "original_list_id": list_data.original_list_id,
        "favorites": [
            {
                "id": f.id,
                "media_id": f.media_id,
                "media_type": f.media_type,
                "title": f.title,
                "poster": f.poster,
                "year": f.year,
                "overview": f.overview,
                "note": f.note,
                "sort_order": f.sort_order
            }
            for f in list_data.favorites
        ] if list_data.favorites else []
    }

    creator = db.query(User).filter(User.id == list_data.user_id).first()
    is_following_creator = False
    
    if current_user:
        follow = db.query(Follow).filter(
            Follow.follower_id == current_user.id,
            Follow.following_id == creator.id
        ).first()
        is_following_creator = follow is not None

    response_data["creator"] = {
        "id": creator.id,
        "username": creator.username,
        "avatar": creator.avatar,
        "is_following": is_following_creator
    }

    if list_data.original_list_id:
        original_list = db.query(FavoriteList).filter(
            FavoriteList.id == list_data.original_list_id
        ).first()
        if original_list:
            original_creator = db.query(User).filter(
                User.id == original_list.user_id
            ).first()
            
            is_following_original = False
            if current_user:
                is_following_original = db.query(Follow).filter(
                    Follow.follower_id == current_user.id,
                    Follow.following_id == original_creator.id
                ).first() is not None
            
            response_data["original_creator"] = {
                "id": original_creator.id,
                "username": original_creator.username,
                "avatar": original_creator.avatar,
                "is_following": is_following_original
            }

    return response_data

@app.put("/api/favorite-lists/{list_id}")
async def update_favorite_list(
    list_id: int,
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        
        favorite_list = db.query(FavoriteList).filter(
            FavoriteList.id == list_id,
            FavoriteList.user_id == current_user.id
        ).first()
        
        if not favorite_list:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—åˆ—è¡¨ä¸å­˜åœ¨æˆ–æ— æƒé™ä¿®æ”¹"
            )
        
        if data.get("name") and data["name"] != favorite_list.name:
            existing_list = db.query(FavoriteList).filter(
                FavoriteList.user_id == current_user.id,
                FavoriteList.name == data["name"],
                FavoriteList.id != list_id
            ).first()
            
            if existing_list:
                raise HTTPException(
                    status_code=400,
                    detail="å·²å­˜åœ¨åŒåæ”¶è—åˆ—è¡¨"
                )
            
            favorite_list.name = data["name"]
        
        if "description" in data:
            favorite_list.description = data["description"]
        
        if "is_public" in data:
            favorite_list.is_public = data["is_public"]
        
        db.commit()
        db.refresh(favorite_list)
        
        return {
            "id": favorite_list.id,
            "name": favorite_list.name,
            "description": favorite_list.description,
            "is_public": favorite_list.is_public,
            "user_id": favorite_list.user_id,
            "created_at": favorite_list.created_at,
            "favorites": [{
                "id": fav.id,
                "media_id": fav.media_id,
                "media_type": fav.media_type,
                "title": fav.title,
                "poster": fav.poster,
                "year": fav.year,
                "overview":fav.overview,
                "note": fav.note
            } for fav in favorite_list.favorites]
        }
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"æ›´æ–°æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.delete("/api/favorite-lists/{list_id}")
async def delete_favorite_list(
    list_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        favorite_list = db.query(FavoriteList).filter(
            FavoriteList.id == list_id,
            FavoriteList.user_id == current_user.id
        ).first()
        
        if not favorite_list:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—åˆ—è¡¨ä¸å­˜åœ¨æˆ–æ— æƒé™åˆ é™¤"
            )
        
        db.delete(favorite_list)
        db.commit()
        
        return {"message": "æ”¶è—åˆ—è¡¨åˆ é™¤æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"åˆ é™¤æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.post("/api/favorite-lists/{list_id}/collect")
async def collect_favorite_list(
    list_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        source_list = db.query(FavoriteList).filter(
            FavoriteList.id == list_id
        ).first()
        
        if not source_list:
            raise HTTPException(
                status_code=404,
                detail="æ”¶è—åˆ—è¡¨ä¸å­˜åœ¨"
            )
            
        if not source_list.is_public:
            raise HTTPException(
                status_code=403,
                detail="è¯¥åˆ—è¡¨ä¸æ˜¯å…¬å¼€åˆ—è¡¨"
            )
            
        new_list = FavoriteList(
            user_id=current_user.id,
            name=f"{source_list.name} (æ”¶è—)",
            description=source_list.description,
            is_public=False,
            original_list_id=list_id
        )
        
        db.add(new_list)
        db.commit()
        db.refresh(new_list)
        
        for fav in source_list.favorites:
            new_favorite = Favorite(
                user_id=current_user.id,
                list_id=new_list.id,
                media_id=fav.media_id,
                media_type=fav.media_type,
                title=fav.title,
                poster=fav.poster,
                year=fav.year,
                overview=fav.overview
            )
            db.add(new_favorite)
            
        db.commit()
        
        return {"message": "æ”¶è—åˆ—è¡¨æˆåŠŸ", "list_id": new_list.id}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.put("/api/favorite-lists/{list_id}/reorder")
async def reorder_favorites(
    list_id: int,
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        data = await request.json()
        favorite_orders = data.get('favorite_ids', [])
        
        favorite_list = db.query(FavoriteList).filter(
            FavoriteList.id == list_id,
            FavoriteList.user_id == current_user.id
        ).first()
        
        if not favorite_list:
            raise HTTPException(status_code=404, detail="æ”¶è—åˆ—è¡¨ä¸å­˜åœ¨æˆ–æ— æƒé™")
        
        for item in favorite_orders:
            favorite = db.query(Favorite).filter(
                Favorite.id == item['id'],
                Favorite.list_id == list_id
            ).first()
            
            if favorite:
                favorite.sort_order = item['sort_order']
        
        db.commit()
        
        updated_favorites = db.query(Favorite).filter(
            Favorite.list_id == list_id
        ).order_by(
            Favorite.sort_order.is_(None),
            Favorite.sort_order,
            Favorite.id
        ).all()
        
        return {
            "message": "æ’åºæ›´æ–°æˆåŠŸ",
            "favorites": [{
                "id": f.id,
                "media_id": f.media_id,
                "media_type": f.media_type,
                "title": f.title,
                "poster": f.poster,
                "year": f.year,
                "overview": f.overview,
                "note": f.note,
                "sort_order": f.sort_order
            } for f in updated_favorites]
        }
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ’åºå¤±è´¥: {str(e)}")

@app.delete("/api/favorite-lists/{list_id}/uncollect")
async def uncollect_favorite_list(
    list_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        collected_list = db.query(FavoriteList).filter(
            FavoriteList.user_id == current_user.id,
            FavoriteList.original_list_id == list_id
        ).first()
        
        if not collected_list:
            raise HTTPException(
                status_code=404,
                detail="æœªæ‰¾åˆ°å·²æ”¶è—çš„åˆ—è¡¨"
            )

        db.delete(collected_list)
        db.commit()
        
        return {"message": "å–æ¶ˆæ”¶è—æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"å–æ¶ˆæ”¶è—å¤±è´¥: {str(e)}"
        )
    
# ==========================================
# 5. ç”¨æˆ·å…³ç³»ç›¸å…³è·¯ç”±
# ==========================================

@app.get("/api/users/{user_id}")
async def get_user_info(
    user_id: int,
    current_user: User = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    try:
        user = db.query(User).filter(User.id == user_id).first()
        
        if not user:
            raise HTTPException(
                status_code=404,
                detail="ç”¨æˆ·ä¸å­˜åœ¨"
            )
            
        is_following = False
        if current_user:
            follow = db.query(Follow).filter(
                Follow.follower_id == current_user.id,
                Follow.following_id == user_id
            ).first()
            
            is_following = follow is not None
        
        return {
            "id": user.id,
            "username": user.username,
            "avatar": user.avatar,
            "email": user.email if current_user and current_user.id == user_id else None,
            "is_following": is_following
        }
    except Exception as e:
        print(f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ç”¨æˆ·ä¿¡æ¯å¤±è´¥: {str(e)}"
        )

@app.get("/api/users/{user_id}/favorite-lists")
async def get_user_favorite_lists(
    user_id: int,
    current_user: User = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    try:
        query = db.query(FavoriteList).filter(FavoriteList.user_id == user_id)
        
        if not current_user or current_user.id != user_id:
            query = query.filter(FavoriteList.is_public == True)
            
        lists = query.all()
        
        result = []
        for list in lists:
            favorites = db.query(Favorite).filter(
                Favorite.list_id == list.id
            ).order_by(
                Favorite.sort_order.is_(None),
                Favorite.sort_order,
                Favorite.id
            ).all()
            
            is_collected = False
            if current_user:
                is_collected = db.query(FavoriteList).filter(
                    FavoriteList.user_id == current_user.id,
                    FavoriteList.original_list_id == list.id
                ).first() is not None
            
            result.append({
                "id": list.id,
                "name": list.name,
                "description": list.description,
                "is_public": list.is_public,
                "is_collected": is_collected,
                "created_at": list.created_at,
                "favorites": [{
                    "id": fav.id,
                    "media_id": fav.media_id,
                    "media_type": fav.media_type,
                    "title": fav.title,
                    "poster": fav.poster,
                    "year": fav.year,
                    "overview": fav.overview,
                    "note": fav.note,
                    "sort_order": fav.sort_order
                } for fav in favorites]
            })
            
        return result
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–ç”¨æˆ·æ”¶è—åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.post("/api/users/{user_id}/follow")
async def follow_user(
    user_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    if current_user.id == user_id:
        raise HTTPException(status_code=400, detail="ä¸èƒ½å…³æ³¨è‡ªå·±")
    
    target_user = db.query(User).filter(User.id == user_id).first()
    if not target_user:
        raise HTTPException(status_code=404, detail="ç”¨æˆ·ä¸å­˜åœ¨")
    
    follow = db.query(Follow).filter(
        Follow.follower_id == current_user.id,
        Follow.following_id == user_id
    ).first()
    
    if follow:
        raise HTTPException(status_code=400, detail="å·²ç»å…³æ³¨è¯¥ç”¨æˆ·")
    
    try:
        new_follow = Follow(
            follower_id=current_user.id,
            following_id=user_id
        )
        
        db.add(new_follow)
        db.commit()
        db.refresh(new_follow)
            
        return {"message": "å…³æ³¨æˆåŠŸ", "is_following": True}
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=500, detail=f"å…³æ³¨å¤±è´¥: {str(e)}")

@app.delete("/api/users/{user_id}/follow")
async def unfollow_user(
    user_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    follow = db.query(Follow).filter(
        Follow.follower_id == current_user.id,
        Follow.following_id == user_id
    ).first()
    
    if not follow:
        raise HTTPException(status_code=404, detail="æœªå…³æ³¨è¯¥ç”¨æˆ·")
    
    try:
        db.delete(follow)
        db.commit()
        return {"message": "å–æ¶ˆå…³æ³¨æˆåŠŸ"}
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=400, detail="å–æ¶ˆå…³æ³¨å¤±è´¥")

@app.get("/api/users/me/following")
async def get_following(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        follows = (
            db.query(Follow)
            .options(selectinload(Follow.following))
            .filter(Follow.follower_id == current_user.id)
            .all()
        )

        return [
            {
                "id": follow.following.id,
                "username": follow.following.username,
                "avatar": follow.following.avatar,
                "note": follow.note,
                "created_at": follow.created_at,
            }
            for follow in follows
            if follow.following is not None
        ]
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è·å–å…³æ³¨åˆ—è¡¨å¤±è´¥: {str(e)}"
        )

@app.put("/api/users/{user_id}/follow/note")
async def update_follow_note(
    user_id: int,
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    try:
        body = await request.json()
        note = body.get("note")
        
        follow = db.query(Follow).filter(
            and_(
                Follow.follower_id == current_user.id,
                Follow.following_id == user_id
            )
        ).first()
        
        if not follow:
            raise HTTPException(status_code=404, detail="æœªå…³æ³¨è¯¥ç”¨æˆ·")
        
        follow.note = note
        db.commit()
        db.refresh(follow)
        
        return {"message": "æ›´æ–°å¤‡æ³¨æˆåŠŸ"}
    except HTTPException:
        raise
    except Exception as e:
        db.rollback()
        raise HTTPException(
            status_code=500,
            detail=f"æ›´æ–°å¤‡æ³¨å¤±è´¥: {str(e)}"
        )

@app.get("/api/users/{user_id}/follow/status")
async def get_follow_status(
    user_id: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    is_following = check_following_status(db, current_user.id, user_id)
    return {"is_following": is_following}

@app.get("/api/debug/follows")
async def debug_follows(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    follows = db.query(Follow).filter(
        Follow.follower_id == current_user.id
    ).all()
    
    return [
        {
            "follower_id": f.follower_id,
            "following_id": f.following_id,
            "created_at": f.created_at
        }
        for f in follows
    ]

# ==========================================
# 6. ä»£ç†å’Œè¾…åŠ©è·¯ç”±
# ==========================================

@app.get("/")
async def root():
    return {"status": "ok", "message": "RateFuse API is running"}

@app.post("/api/ratings/batch")
async def get_batch_ratings(
    request: Request, 
    db: Session = Depends(get_db),
    current_user: Optional[User] = Depends(get_current_user_optional)
):
    """æ‰¹é‡è·å–å¤šä¸ªå½±è§†çš„è¯„åˆ†ä¿¡æ¯"""
    start_time = time.time()
    try:
        body = await request.json()
        items = body.get('items', [])
        max_concurrent = body.get('max_concurrent', 5)
        
        if not items or len(items) == 0:
            raise HTTPException(status_code=400, detail="itemsä¸èƒ½ä¸ºç©º")
        
        if len(items) > 50:
            raise HTTPException(status_code=400, detail="å•æ¬¡æœ€å¤šæ”¯æŒ50ä¸ªå½±è§†")
        
        logger.info(f"\n{'='*60}\n  æ‰¹é‡è·å–è¯„åˆ† | æ•°é‡: {len(items)} | å¹¶å‘: {max_concurrent}\n{'='*60}")
        
        douban_cookie = None
        if current_user:
            if current_user.douban_cookie:
                douban_cookie = current_user.douban_cookie
                print(f"âœ… å·²è·å–ç”¨æˆ· {current_user.id} çš„è±†ç“£Cookieï¼ˆé•¿åº¦: {len(douban_cookie)}ï¼‰")
            else:
                print(f"âš ï¸ ç”¨æˆ· {current_user.id} æœªè®¾ç½®è±†ç“£Cookie")
        else:
            print("âš ï¸ æœªç™»å½•ç”¨æˆ·ï¼Œæ— æ³•ä½¿ç”¨è±†ç“£Cookie")
        
        async def get_item_info(item):
            media_id = item['id']
            media_type = item['type']
            
            cache_key = f"ratings:all:{media_type}:{media_id}"
            cached = await get_cache(cache_key)
            if cached:
                return media_id, {'cached': True, 'data': cached}
            
            try:
                tmdb_info = await get_tmdb_info_cached(media_id, media_type, request)
                if not tmdb_info:
                    return media_id, {'error': 'TMDBä¿¡æ¯è·å–å¤±è´¥'}
                
                return media_id, {'tmdb_info': tmdb_info, 'type': media_type}
            except Exception as e:
                return media_id, {'error': str(e)}
        
        tmdb_tasks = [get_item_info(item) for item in items]
        tmdb_results = await asyncio.gather(*tmdb_tasks, return_exceptions=True)
        
        cached_results = {}
        to_fetch = []
        errors = {}
        
        for result in tmdb_results:
            if isinstance(result, Exception):
                continue
            media_id, data = result
            if data.get('cached'):
                cached_results[media_id] = data['data']
            elif 'tmdb_info' in data:
                to_fetch.append((media_id, data['tmdb_info'], data['type']))
            elif 'error' in data:
                errors[media_id] = data['error']
        
        logger.info(f"ğŸ“Š ç¼“å­˜: {len(cached_results)} | çˆ¬å–: {len(to_fetch)} | é”™è¯¯: {len(errors)}")
        
        sem = asyncio.Semaphore(max_concurrent)
        
        async def fetch_one_item(media_id, tmdb_info, media_type):
            async with sem:
                try:
                    item_start = time.time()
                    title = tmdb_info.get('zh_title') or tmdb_info.get('title', media_id)
                    logger.info(f"  â†’ {title[:30]}... (ID: {media_id})")
                    
                    from ratings import parallel_extract_ratings
                    
                    ratings = await asyncio.wait_for(
                        parallel_extract_ratings(tmdb_info, media_type, request, douban_cookie),
                        timeout=20.0
                    )
                    
                    cache_key = f"ratings:all:{media_type}:{media_id}"
                    if ratings:
                        await set_cache(cache_key, ratings, expire=CACHE_EXPIRE_TIME)
                    
                    item_time = time.time() - item_start
                    logger.info(f"  âœ“ {media_id}: {item_time:.1f}s")
                    
                    return media_id, {'ratings': ratings, 'status': 'success', 'time': item_time}
                    
                except asyncio.TimeoutError:
                    logger.warning(f"  â± {media_id}: è¶…æ—¶")
                    return media_id, {'status': 'timeout', 'error': 'è·å–è¶…æ—¶ï¼ˆ>20ç§’ï¼‰'}
                except Exception as e:
                    logger.error(f"  âœ— {media_id}: {str(e)[:30]}")
                    return media_id, {'status': 'error', 'error': str(e)}
        
        if to_fetch:
            fetch_tasks = [
                fetch_one_item(media_id, tmdb_info, media_type)
                for media_id, tmdb_info, media_type in to_fetch
            ]
            fetch_results = await asyncio.gather(*fetch_tasks, return_exceptions=True)
        else:
            fetch_results = []
        
        final_results = {}
        
        for media_id, data in cached_results.items():
            final_results[media_id] = {
                'ratings': data,
                'status': 'success',
                'from_cache': True
            }
        
        for result in fetch_results:
            if isinstance(result, Exception):
                continue
            media_id, data = result
            final_results[media_id] = data
        
        for media_id, error in errors.items():
            final_results[media_id] = {
                'status': 'error',
                'error': error
            }
        
        total_time = time.time() - start_time
        success_count = sum(1 for r in final_results.values() if r.get('status') == 'success')
        
        logger.info(f"\n{'='*60}")
        logger.info(f"  âœ“ æ‰¹é‡å®Œæˆ: {success_count}/{len(items)} ä¸ª | æ€»è€—æ—¶: {total_time:.1f}s | å¹³å‡: {total_time/len(items):.1f}s/ä¸ª")
        logger.info(f"{'='*60}\n")
        
        return {
            'results': final_results,
            '_performance': {
                'total_time': round(total_time, 2),
                'total_items': len(items),
                'cached_items': len(cached_results),
                'fetched_items': len(to_fetch),
                'error_items': len(errors),
                'avg_time_per_item': round(total_time / len(items), 2) if items else 0,
                'max_concurrent': max_concurrent
            }
        }
    
    except HTTPException:
        raise
    except Exception as e:
        print(f"æ‰¹é‡è·å–è¯„åˆ†å¤±è´¥: {e}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"æ‰¹é‡è·å–å¤±è´¥: {str(e)}")

@app.get("/api/ratings/all/{type}/{id}")
async def get_all_platform_ratings(
    type: str, 
    id: str, 
    request: Request,
    current_user: Optional[User] = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    """å¹¶è¡Œè·å–æ‰€æœ‰å¹³å°çš„è¯„åˆ†ä¿¡æ¯"""
    start_time = time.time()
    try:
        if await request.is_disconnected():
            print("è¯·æ±‚å·²åœ¨å¼€å§‹æ—¶è¢«å–æ¶ˆ")
            return None

        douban_cookie = None
        if current_user:
            if current_user.douban_cookie:
                douban_cookie = current_user.douban_cookie
                print(f"âœ… å·²è·å–ç”¨æˆ· {current_user.id} çš„è±†ç“£Cookieï¼ˆé•¿åº¦: {len(douban_cookie)}ï¼‰")
            else:
                print(f"âš ï¸ ç”¨æˆ· {current_user.id} æœªè®¾ç½®è±†ç“£Cookie")
        else:
            print("âš ï¸ æœªç™»å½•ç”¨æˆ·ï¼Œæ— æ³•ä½¿ç”¨è±†ç“£Cookie")
        
        cache_key = f"ratings:all:{type}:{id}"
        cached_data = await get_cache(cache_key)
        if cached_data:
            print(f"ä»ç¼“å­˜è·å–æ‰€æœ‰å¹³å°è¯„åˆ†æ•°æ®ï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
            return cached_data
        
        tmdb_info = await get_tmdb_info_cached(id, type, request)
        if not tmdb_info:
            if await request.is_disconnected():
                print("è¯·æ±‚åœ¨è·å–TMDBä¿¡æ¯æ—¶è¢«å–æ¶ˆ")
                return None
            raise HTTPException(status_code=404, detail="æ— æ³•è·å– TMDB ä¿¡æ¯")
        
        if await request.is_disconnected():
            print("è¯·æ±‚åœ¨è·å–TMDBä¿¡æ¯åè¢«å–æ¶ˆ")
            return None
        
        from ratings import parallel_extract_ratings
        
        try:
            all_ratings = await asyncio.wait_for(
                parallel_extract_ratings(tmdb_info, tmdb_info["type"], request, douban_cookie),
                timeout=20.0
            )
        except asyncio.TimeoutError:
            logger.error("è·å–è¯„åˆ†è¶…æ—¶ï¼ˆ>20ç§’ï¼‰")
            raise HTTPException(status_code=504, detail="è·å–è¯„åˆ†è¶…æ—¶")
        
        if await request.is_disconnected():
            return None
        
        total_time = time.time() - start_time
        
        if all_ratings:
            await set_cache(cache_key, all_ratings, expire=CACHE_EXPIRE_TIME)
        
        result = {
            "ratings": all_ratings,
            "_performance": {
                "total_time": round(total_time, 2),
                "cached": False,
                "parallel": True
            }
        }
        
        return result
    
    except HTTPException:
        raise
    except Exception as e:
        if await request.is_disconnected():
            return None
        
        logger.error(f"è·å–æ‰€æœ‰å¹³å°è¯„åˆ†å¤±è´¥: {str(e)[:100]}")
        raise HTTPException(status_code=500, detail=f"è·å–è¯„åˆ†å¤±è´¥: {str(e)}")

@app.get("/api/ratings/{platform}/{type}/{id}")
async def get_platform_rating(
    platform: str, 
    type: str, 
    id: str, 
    request: Request,
    current_user: Optional[User] = Depends(get_current_user_optional),
    db: Session = Depends(get_db)
):
    """è·å–æŒ‡å®šå¹³å°çš„è¯„åˆ†ä¿¡æ¯ï¼Œä¼˜åŒ–ç¼“å­˜å’Œé”™è¯¯å¤„ç†"""
    start_time = time.time()
    try:
        if await request.is_disconnected():
            print(f"{platform} è¯·æ±‚å·²åœ¨å¼€å§‹æ—¶è¢«å–æ¶ˆ")
            return None
        
        douban_cookie = None
        if platform == "douban":
            if current_user:
                if current_user.douban_cookie:
                    douban_cookie = current_user.douban_cookie
                    print(f"âœ… å·²è·å–ç”¨æˆ· {current_user.id} çš„è±†ç“£Cookieï¼ˆé•¿åº¦: {len(douban_cookie)}ï¼‰")
                else:
                    print(f"âš ï¸ ç”¨æˆ· {current_user.id} æœªè®¾ç½®è±†ç“£Cookie")
            else:
                print("âš ï¸ æœªç™»å½•ç”¨æˆ·ï¼Œæ— æ³•ä½¿ç”¨è±†ç“£Cookie")
        
        cache_key = f"rating:{platform}:{type}:{id}"
        cached_data = await get_cache(cache_key)
        if cached_data:
            print(f"ä»ç¼“å­˜è·å– {platform} è¯„åˆ†æ•°æ®ï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
            return cached_data

        tmdb_info = await get_tmdb_info_cached(id, type, request)
        if not tmdb_info:
            if await request.is_disconnected():
                print(f"{platform} è¯·æ±‚åœ¨è·å–TMDBä¿¡æ¯æ—¶è¢«å–æ¶ˆ")
                return None
            raise HTTPException(status_code=404, detail="æ— æ³•è·å– TMDB ä¿¡æ¯")

        if await request.is_disconnected():
            print(f"{platform} è¯·æ±‚åœ¨è·å–TMDBä¿¡æ¯åè¢«å–æ¶ˆ")
            return None

        search_start_time = time.time()
        search_results = await search_platform(platform, tmdb_info, request, douban_cookie)

        if isinstance(search_results, dict) and search_results.get("status") in (
            RATING_STATUS["NO_FOUND"],
            RATING_STATUS["RATE_LIMIT"],
            RATING_STATUS["TIMEOUT"],
            RATING_STATUS["FETCH_FAILED"],
        ):
            reason = search_results.get("status_reason") or search_results.get("status")
            rating_info = create_rating_data(search_results["status"], reason)
            rating_info["_performance"] = {
                "total_time": round(time.time() - start_time, 2),
                "search_time": round(time.time() - search_start_time, 2),
                "extract_time": 0,
                "cached": False,
            }
            return rating_info

        if await request.is_disconnected():
            print(f"{platform} è¯·æ±‚åœ¨æœç´¢å¹³å°åè¢«å–æ¶ˆ")
            return None

        if isinstance(search_results, dict) and search_results.get("status") == "cancelled":
            print(f"{platform} æœç´¢è¢«å–æ¶ˆ")
            return None

        extract_start_time = time.time()
        rating_info = await extract_rating_info(type, platform, tmdb_info, search_results, request, douban_cookie)

        if await request.is_disconnected():
            print(f"{platform} è¯·æ±‚åœ¨è·å–è¯„åˆ†ä¿¡æ¯åè¢«å–æ¶ˆ")
            return None

        if not rating_info:
            if await request.is_disconnected():
                print(f"{platform} è¯·æ±‚åœ¨å¤„ç†è¯„åˆ†ä¿¡æ¯æ—¶è¢«å–æ¶ˆ")
                return None
            raise HTTPException(status_code=404, detail=f"æœªæ‰¾åˆ° {platform} çš„è¯„åˆ†ä¿¡æ¯")

        if isinstance(rating_info, dict) and rating_info.get("status") == "cancelled":
            print(f"{platform} è¯„åˆ†æå–è¢«å–æ¶ˆ")
            return None

        if isinstance(rating_info, dict) and rating_info.get("status") == RATING_STATUS["SUCCESSFUL"]:
            await set_cache(cache_key, rating_info)
            print(f"å·²ç¼“å­˜ {platform} è¯„åˆ†æ•°æ®")
        else:
            print(f"ä¸ç¼“å­˜ {platform} è¯„åˆ†æ•°æ®ï¼ŒçŠ¶æ€: {rating_info.get('status')}")

        total_time = time.time() - start_time
        
        if isinstance(rating_info, dict):
            rating_info["_performance"] = {
                "total_time": round(total_time, 2),
                "search_time": round(time.time() - search_start_time, 2),
                "extract_time": round(time.time() - extract_start_time, 2),
                "cached": False
            }

        return rating_info

    except HTTPException:
        raise
    except Exception as e:
        if await request.is_disconnected():
            print(f"{platform} è¯·æ±‚åœ¨å‘ç”Ÿé”™è¯¯æ—¶è¢«å–æ¶ˆ")
            return None
        
        print(f"è·å– {platform} è¯„åˆ†æ—¶å‡ºé”™: {str(e)}")
        print(traceback.format_exc())
        raise HTTPException(status_code=500, detail=f"è·å–è¯„åˆ†å¤±è´¥: {str(e)}")
    finally:
        print(f"{platform} è¯·æ±‚å¤„ç†å®Œæˆï¼Œæ€»è€—æ—¶: {time.time() - start_time:.2f}ç§’")

router = APIRouter()

_tmdb_client = None

async def get_tmdb_client():
    """è·å–æˆ–åˆ›å»º TMDB API å®¢æˆ·ç«¯"""
    global _tmdb_client
    if _tmdb_client is None or _tmdb_client.is_closed:
        _tmdb_client = httpx.AsyncClient(
            http2=True,
            timeout=httpx.Timeout(10.0),
            limits=httpx.Limits(
                max_connections=100,
                max_keepalive_connections=20,
                keepalive_expiry=30.0
            ),
            headers={
                "accept": "application/json",
                "accept-encoding": "gzip, deflate",
                "Authorization": "Bearer eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI0ZjY4MWZhN2I1YWI3MzQ2YTRlMTg0YmJmMmQ0MTcxNSIsIm5iZiI6MTUyNjE3NDY5MC4wMjksInN1YiI6IjVhZjc5M2UyOTI1MTQxMmM4MDAwNGE5ZCIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.maKS7ZH7y6l_H0_dYcXn5QOZHuiYdK_SsiQ5AAk32cI"
            }
        )
    return _tmdb_client

def _normalized_query_for_cache(query_params) -> str:
    """å°† query å‚æ•°æŒ‰ key æ’åºåæ‹¼æ¥"""
    if not query_params:
        return ""
    sorted_items = sorted(query_params.items(), key=lambda x: x[0])
    return "&".join(f"{k}={v}" for k, v in sorted_items)

_tmdb_search_times: dict[str, list[float]] = {}
_tmdb_rate_lock = asyncio.Lock()
TMDB_SEARCH_LIMIT = 10
TMDB_SEARCH_WINDOW = 10.0


async def _check_tmdb_search_rate_limit(client_ip: str) -> None:
    """ä»…å¯¹ search è·¯å¾„é™æµï¼Œè¶…é™æŠ› 429"""
    if not client_ip:
        return
    now = time.time()
    async with _tmdb_rate_lock:
        if client_ip not in _tmdb_search_times:
            _tmdb_search_times[client_ip] = []
        times = _tmdb_search_times[client_ip]
        times[:] = [t for t in times if now - t < TMDB_SEARCH_WINDOW]
        if len(times) >= TMDB_SEARCH_LIMIT:
            raise HTTPException(status_code=429, detail="TMDB æœç´¢è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•")
        times.append(now)


@router.get("/api/tmdb-proxy/{path:path}")
async def tmdb_proxy(path: str, request: Request):
    """ä»£ç† TMDB API è¯·æ±‚å¹¶ç¼“å­˜ç»“æœ"""
    try:
        qs = _normalized_query_for_cache(dict(request.query_params))
        cache_key = f"tmdb:{path}:{qs}"
        cached_data = await get_cache(cache_key)
        if cached_data:
            return cached_data
        if path.strip("/").startswith("search"):
            client_ip = request.client.host if request.client else ""
            forwarded = request.headers.get("x-forwarded-for")
            if forwarded:
                client_ip = forwarded.split(",")[0].strip()
            await _check_tmdb_search_rate_limit(client_ip)
        
        params = dict(request.query_params)
        tmdb_url = f"https://api.themoviedb.org/3/{path}"
        client = await get_tmdb_client()
        
        try:
            response = await client.get(tmdb_url, params=params)
            
            if response.status_code != 200:
                try:
                    err_json = response.json()
                except Exception:
                    err_json = {"error": response.text}
                raise HTTPException(status_code=response.status_code, detail={
                    "message": "TMDB API è¯·æ±‚å¤±è´¥",
                    "status": response.status_code,
                    "body": err_json
                })
            
            data = response.json()
            await set_cache(cache_key, data)
            
            return data
            
        except httpx.TimeoutException:
            raise HTTPException(status_code=504, detail="TMDB API è¯·æ±‚è¶…æ—¶")
        except httpx.HTTPError as e:
            raise HTTPException(status_code=500, detail=f"HTTP è¯·æ±‚é”™è¯¯: {str(e)}")
                
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»£ç†è¯·æ±‚å¤±è´¥: {str(e)}")

@app.get("/api/image-proxy")
async def image_proxy(url: str, response: Response):
    """ä»£ç†å›¾ç‰‡è¯·æ±‚å¹¶æ·»åŠ ç¼“å­˜æ§åˆ¶"""
    try:
        if url.startswith('/tmdb-images/'):
            url = f"https://image.tmdb.org/t/p{url[12:]}"
        
        cache_key = f"img:{url}"
        
        if redis:
            try:
                cached_url = await redis.get(cache_key)
                if cached_url:
                    response.headers["Location"] = cached_url.decode('utf-8')
                    response.status_code = 302
                    return
            except Exception as redis_error:
                print(f"Redisç¼“å­˜é”™è¯¯: {str(redis_error)}")
        
        etag = 'W/"' + hashlib.md5(url.encode('utf-8')).hexdigest() + '"'
        inm = None
        try:
            inm = response.headers.get('If-None-Match')
        except Exception:
            pass

        cached_item = None
        if redis:
            try:
                cached_raw = await redis.get(f"imgbin:{url}")
                if cached_raw:
                    cached_item = json.loads(cached_raw)
            except Exception:
                cached_item = None

        if cached_item:
            img_bytes = base64.b64decode(cached_item.get('data', ''))
            content_type = cached_item.get('content_type', 'image/jpeg')
            headers = {
                "Cache-Control": "public, max-age=604800, immutable",
                "ETag": etag,
                "Content-Type": content_type,
            }
            return Response(content=img_bytes, media_type=content_type, headers=headers)

        async with aiohttp.ClientSession(connector=aiohttp.TCPConnector(ssl=False)) as session:
            try:
                async with session.get(url, timeout=10) as img_response:
                    if img_response.status != 200:
                        print(f"å›¾ç‰‡è·å–å¤±è´¥ï¼ŒçŠ¶æ€ç : {img_response.status}, URL: {url}")
                        raise HTTPException(status_code=img_response.status, detail="å›¾ç‰‡è·å–å¤±è´¥")
                    
                    content_type = img_response.headers.get("Content-Type", "image/jpeg")
                    image_data = await img_response.read()
                    
                    if redis:
                        try:
                            await redis.setex(
                                f"imgbin:{url}",
                                7 * 24 * 60 * 60,
                                json.dumps({
                                    'content_type': content_type,
                                    'data': base64.b64encode(image_data).decode('utf-8')
                                })
                            )
                        except Exception:
                            pass

                    headers = {
                        "Cache-Control": "public, max-age=604800, immutable",
                        "ETag": etag,
                        "Content-Type": content_type,
                    }
                    return Response(content=image_data, media_type=content_type, headers=headers)
            except aiohttp.ClientError as client_error:
                print(f"AIOHTTPå®¢æˆ·ç«¯é”™è¯¯: {str(client_error)}, URL: {url}")
                raise HTTPException(status_code=500, detail=f"å›¾ç‰‡è·å–å¤±è´¥: {str(client_error)}")
            except asyncio.TimeoutError:
                print(f"è¯·æ±‚è¶…æ—¶: URL: {url}")
                raise HTTPException(status_code=504, detail="å›¾ç‰‡è¯·æ±‚è¶…æ—¶")
                
    except Exception as e:
        print(f"å›¾ç‰‡ä»£ç†å¤±è´¥: {str(e)}, URL: {url}")
        raise HTTPException(status_code=500, detail=f"å›¾ç‰‡ä»£ç†å¤±è´¥: {str(e)}")

@router.get("/api/trakt-proxy/{path:path}")
async def trakt_proxy(path: str, request: Request):
    """ä»£ç† Trakt API è¯·æ±‚å¹¶ç¼“å­˜ç»“æœ"""
    try:
        cache_key = f"trakt:{path}:{request.query_params}"
        cached_data = await get_cache(cache_key)
        if cached_data:
            return cached_data
        
        trakt_url = f"https://api.trakt.tv/{path}"
        params = dict(request.query_params)
        headers = {
            'Content-Type': 'application/json',
            'trakt-api-version': '2',
            'trakt-api-key': 'db74b025288459dc36589f6207fb96aabd83be8ea5d502810a049c29ffd9bff0'
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.get(trakt_url, params=params, headers=headers) as response:
                if response.status != 200:
                    return HTTPException(status_code=response.status, detail="Trakt API è¯·æ±‚å¤±è´¥")
                
                data = await response.json()
                await set_cache(cache_key, data)
                
                return data
                
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä»£ç†è¯·æ±‚å¤±è´¥: {str(e)}")

app.include_router(router)
# ==========================================
# 6.1 æ‰‹å·¥æ¦œå•å½•å…¥ä¸èšåˆï¼ˆç®¡ç†å‘˜ï¼‰
# ==========================================

def require_admin(user: User):
    if not user.is_admin:
        raise HTTPException(status_code=403, detail="éœ€è¦ç®¡ç†å‘˜æƒé™")

async def tmdb_enrich(tmdb_id: int, media_type: str):
    """ä½¿ç”¨å¤šè¯­è¨€å›é€€è·å–TMDBä¿¡æ¯"""
    from ratings import _fetch_tmdb_with_language_fallback, get_tmdb_http_client
    
    try:
        client = get_tmdb_http_client()
        endpoint = f"https://api.themoviedb.org/3/{media_type}/{tmdb_id}"
        
        data = await _fetch_tmdb_with_language_fallback(client, endpoint)
        
        if not data:
            raise HTTPException(status_code=400, detail="TMDB ä¿¡æ¯è·å–å¤±è´¥")
        
        title = data.get("title") if media_type == "movie" else data.get("name")
        poster_path = data.get("poster_path")
        poster = f"/tmdb-images/w500{poster_path}" if poster_path else ""
        original_language = data.get("original_language", "")
        
        return {
            "title": title or "", 
            "poster": poster or "", 
            "original_language": original_language or ""
        }
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"TMDB ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")

@app.post("/api/charts/entries")
async def add_chart_entry(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    require_admin(current_user)
    body = await request.json()
    platform = body.get("platform")
    chart_name = body.get("chart_name")
    media_type = body.get("media_type")
    tmdb_id = body.get("tmdb_id")
    rank = body.get("rank")
    title = body.get("title")
    poster = body.get("poster")

    if not (platform and chart_name and media_type in ("movie","tv") and isinstance(tmdb_id, int) and isinstance(rank, int)):
        raise HTTPException(status_code=400, detail="å‚æ•°ä¸å®Œæ•´")

    enrich = await tmdb_enrich(tmdb_id, media_type)
    title = title or enrich["title"]
    poster = poster or enrich["poster"]
    original_language = enrich["original_language"]

    try:
        existing = db.query(ChartEntry).filter(
            ChartEntry.platform == platform,
            ChartEntry.chart_name == chart_name,
            ChartEntry.media_type == media_type,
            ChartEntry.rank == rank,
        ).first()
        if existing:
            if existing.locked:
                raise HTTPException(status_code=423, detail="è¯¥æ’åå·²é”å®šï¼Œæ— æ³•ä¿®æ”¹")
            existing.tmdb_id = tmdb_id
            existing.title = title
            existing.poster = poster
            existing.original_language = original_language
            existing.created_by = current_user.id
            db.commit()
            db.refresh(existing)
            return {"id": existing.id, "updated": True}

        entry = ChartEntry(
            platform=platform,
            chart_name=chart_name,
            media_type=media_type,
            tmdb_id=tmdb_id,
            title=title,
            poster=poster,
            rank=rank,
            original_language=original_language,
            created_by=current_user.id,
        )
        db.add(entry)
        db.commit()
        db.refresh(entry)
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=400, detail=f"ä¿å­˜å¤±è´¥æˆ–é‡å¤: {str(e)}")
    return {"id": entry.id}

@app.post("/api/charts/entries/bulk")
async def add_chart_entries_bulk(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ‰¹é‡å½•å…¥æ¦œå•"""
    require_admin(current_user)
    items = (await request.json()).get("items", [])
    if not isinstance(items, list) or not items:
        raise HTTPException(status_code=400, detail="items å¿…é¡»æ˜¯éç©ºæ•°ç»„")

    valid = []
    for item in items:
        platform = item.get("platform")
        chart_name = item.get("chart_name")
        media_type = item.get("media_type")
        tmdb_id = item.get("tmdb_id")
        rank = item.get("rank")
        title = item.get("title")
        poster = item.get("poster")
        if not (platform and chart_name and media_type in ("movie", "tv") and isinstance(tmdb_id, int) and isinstance(rank, int)):
            continue
        try:
            enrich = await tmdb_enrich(tmdb_id, media_type)
            title = title or enrich["title"]
            poster = poster or enrich["poster"]
            original_language = enrich.get("original_language", "")
            valid.append(ChartEntry(
                platform=platform,
                chart_name=chart_name,
                media_type=media_type,
                tmdb_id=tmdb_id,
                title=title,
                poster=poster,
                rank=rank,
                original_language=original_language,
                created_by=current_user.id,
            ))
        except Exception:
            continue
    if not valid:
        return {"created": []}
    try:
        db.add_all(valid)
        db.commit()
        for e in valid:
            db.refresh(e)
        created = [e.id for e in valid]
        if redis:
            try:
                await redis.delete("charts:aggregate", "charts:public")
            except Exception:
                pass
        return {"created": created}
    except Exception as e:
        db.rollback()
        raise HTTPException(status_code=400, detail=f"æ‰¹é‡ä¿å­˜å¤±è´¥: {str(e)}")

def aggregate_top(
    db: Session,
    media_type: str,
    limit: int = 10,
    chinese_only: bool = False,
    include_pairs: list[tuple[str, str]] | None = None,
):
    sub = db.query(
        ChartEntry.platform,
        ChartEntry.chart_name,
        ChartEntry.media_type,
        ChartEntry.rank,
        func.max(ChartEntry.id).label('max_id')
    ).group_by(ChartEntry.platform, ChartEntry.chart_name, ChartEntry.media_type, ChartEntry.rank).subquery()

    entries = db.query(ChartEntry).join(
        sub,
        (ChartEntry.id == sub.c.max_id)
    ).filter(ChartEntry.media_type == media_type)
    if chinese_only:
        entries = entries.filter(ChartEntry.original_language == "zh")
    if include_pairs:
        conditions = []
        for plat, chart in include_pairs:
            conditions.append(and_(ChartEntry.platform == plat, ChartEntry.chart_name == chart))
        if conditions:
            entries = entries.filter(or_(*conditions))
    entries = entries.all()
    freq: dict[int, int] = {}
    best_rank: dict[int, int] = {}
    latest_id: dict[int, int] = {}
    sample: dict[int, ChartEntry] = {}
    for e in entries:
        key = int(e.tmdb_id)
        freq[key] = freq.get(key, 0) + 1
        best_rank[key] = min(best_rank.get(key, 9999), int(e.rank) if e.rank is not None else 9999)
        latest_id[key] = max(latest_id.get(key, 0), int(e.id))
        if key not in sample:
            sample[key] = e
    ranked_keys = sorted(freq.keys(), key=lambda k: (-freq[k], best_rank[k], -latest_id[k], k))
    result = []
    for tmdb_id in ranked_keys[:limit]:
        e = sample[int(tmdb_id)]
        poster = e.poster or ""
        if poster and not (poster.startswith("/tmdb-images/") or poster.startswith("/api/") or poster.startswith("http")):
            if not poster.startswith("/"):
                poster = "/" + poster
            poster = f"/tmdb-images{poster}"
        result.append({"id": e.tmdb_id, "type": media_type, "title": e.title, "poster": poster})
    return result

def latest_chart_top_by_rank(
    db: Session,
    platform: str,
    chart_name: str,
    media_type: str,
    limit: int = 10,
):
    sub = db.query(
        ChartEntry.rank,
        func.max(ChartEntry.id).label('max_id')
    ).filter(
        ChartEntry.platform == platform,
        ChartEntry.chart_name == chart_name,
        ChartEntry.media_type == media_type,
    ).group_by(ChartEntry.rank).subquery()

    rows = db.query(ChartEntry).join(sub, ChartEntry.id == sub.c.max_id).order_by(ChartEntry.rank.asc()).limit(limit).all()
    result = []
    for e in rows:
        poster = e.poster or ""
        if poster and not (poster.startswith("/tmdb-images/") or poster.startswith("/api/") or poster.startswith("http")):
            if not poster.startswith("/"):
                poster = "/" + poster
            poster = f"/tmdb-images{poster}"
        result.append({
            "id": e.tmdb_id,
            "type": media_type,
            "title": e.title,
            "poster": poster,
        })
    return result

@app.get("/api/charts/entries")
async def list_chart_entries(
    platform: str,
    chart_name: str,
    media_type: str,
    db: Session = Depends(get_db)
):
    if media_type not in ("movie", "tv"):
        raise HTTPException(status_code=400, detail="media_type å¿…é¡»ä¸º movie æˆ– tv")
    items = (
        db.query(ChartEntry)
        .filter(
            ChartEntry.platform == platform,
            ChartEntry.chart_name == chart_name,
            ChartEntry.media_type == media_type,
        )
        .order_by(ChartEntry.rank.asc())
        .limit(500)
        .all()
    )
    return [
        {
            "id": e.id,
            "tmdb_id": e.tmdb_id,
            "rank": e.rank,
            "title": e.title,
            "poster": e.poster,
            "locked": e.locked,
        }
        for e in items
    ]

@app.put("/api/charts/entries/lock")
async def set_entry_lock(
    platform: str,
    chart_name: str,
    media_type: str,
    rank: int,
    locked: bool,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    entry = db.query(ChartEntry).filter(
        ChartEntry.platform == platform,
        ChartEntry.chart_name == chart_name,
        ChartEntry.media_type == media_type,
        ChartEntry.rank == rank,
    ).first()
    if not entry:
        raise HTTPException(status_code=404, detail="æ¡ç›®ä¸å­˜åœ¨")
    entry.locked = locked
    db.commit()
    return {"rank": rank, "locked": locked}

@app.delete("/api/charts/entries")
async def delete_chart_entry(
    platform: str,
    chart_name: str,
    media_type: str,
    rank: int,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    entry = db.query(ChartEntry).filter(
        ChartEntry.platform == platform,
        ChartEntry.chart_name == chart_name,
        ChartEntry.media_type == media_type,
        ChartEntry.rank == rank,
    ).first()
    if not entry:
        raise HTTPException(status_code=404, detail="æ¡ç›®ä¸å­˜åœ¨")
    if entry.locked:
        raise HTTPException(status_code=423, detail="è¯¥æ’åå·²é”å®šï¼Œæ— æ³•åˆ é™¤")
    db.delete(entry)
    db.commit()
    return {"deleted": True, "rank": rank}

@app.get("/api/charts/aggregate")
async def get_aggregate_charts(db: Session = Depends(get_db)):
    cache_key = "charts:aggregate"
    cached = await get_cache(cache_key)
    if cached:
        return cached

    chinese_tv = latest_chart_top_by_rank(
        db,
        platform="è±†ç“£",
        chart_name="ä¸€å‘¨åè¯­å‰§é›†å£ç¢‘æ¦œ",
        media_type="tv",
        limit=10,
    )

    movie_include_pairs = [
        ("è±†ç“£", "ä¸€å‘¨å£ç¢‘æ¦œ"),
        ("IMDb", "Top 10 on IMDb this week"),
        ("çƒ‚ç•ªèŒ„", "Popular Streaming Movies"),
        ("MTC", "Trending Movies This Week"),
        ("Letterboxd", "Popular films this week"),
        ("TMDB", "è¶‹åŠ¿æœ¬å‘¨"),
        ("Trakt", "Top Movies Last Week"),
    ]
    
    tv_include_pairs = [
        ("è±†ç“£", "ä¸€å‘¨å…¨çƒå‰§é›†å£ç¢‘æ¦œ"),
        ("çƒ‚ç•ªèŒ„", "Popular TV"),
        ("MTC", "Trending Shows This Week"),
        ("Letterboxd", "Popular films this week"),
        ("TMDB", "è¶‹åŠ¿æœ¬å‘¨"),
        ("Trakt", "Top TV Shows Last Week"),
    ]
    
    movies = aggregate_top(db, media_type="movie", limit=10, chinese_only=False, include_pairs=movie_include_pairs)
    tv_candidates = aggregate_top(db, media_type="tv", limit=50, chinese_only=False, include_pairs=tv_include_pairs)
    tv = tv_candidates[:10]
    result = {"top_movies": movies, "top_tv": tv, "top_chinese_tv": chinese_tv}
    await set_cache(cache_key, result, expire=CHARTS_CACHE_EXPIRE)
    return result

@app.post("/api/charts/sync")
async def sync_charts(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åŒæ­¥æ¦œå•æ•°æ®åˆ°å…¬å¼€é¡µé¢ï¼ˆå°†ChartEntryæ•°æ®å¤åˆ¶åˆ°PublicChartEntryï¼‰"""
    require_admin(current_user)
    
    try:
        db.query(PublicChartEntry).delete()
        db.commit()
        
        distinct_charts = db.query(
            ChartEntry.platform,
            ChartEntry.chart_name,
            ChartEntry.media_type
        ).distinct().all()
        
        synced_count = 0
        synced_at = datetime.utcnow()
        
        for platform, chart_name, media_type in distinct_charts:
            sub = db.query(
                ChartEntry.rank,
                func.max(ChartEntry.id).label('max_id')
            ).filter(
                ChartEntry.platform == platform,
                ChartEntry.chart_name == chart_name,
                ChartEntry.media_type == media_type,
            ).group_by(ChartEntry.rank).subquery()
            
            entries = db.query(ChartEntry).join(
                sub, ChartEntry.id == sub.c.max_id
            ).order_by(ChartEntry.rank.asc()).all()
            
            for entry in entries:
                public_entry = PublicChartEntry(
                    platform=entry.platform,
                    chart_name=entry.chart_name,
                    media_type=entry.media_type,
                    tmdb_id=entry.tmdb_id,
                    title=entry.title,
                    poster=entry.poster,
                    rank=entry.rank,
                    synced_at=synced_at
                )
                db.add(public_entry)
                synced_count += 1
        
        db.commit()
        if redis:
            try:
                await redis.delete("charts:aggregate", "charts:public")
            except Exception:
                pass
        return {
            "status": "success",
            "message": f"æ¦œå•æ•°æ®å·²åŒæ­¥ï¼Œå…± {synced_count} æ¡è®°å½•",
            "total_count": synced_count,
            "timestamp": synced_at.isoformat()
        }
    except Exception as e:
        db.rollback()
        logger.error(f"åŒæ­¥æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åŒæ­¥æ¦œå•å¤±è´¥: {str(e)}")

@app.get("/api/charts/public")
async def get_public_charts(db: Session = Depends(get_db)):
    """è·å–æ‰€æœ‰å…¬å¼€æ¦œå•æ•°æ®"""
    cache_key = "charts:public"
    cached = await get_cache(cache_key)
    if cached is not None:
        return cached
    try:
        metacritic_top250_charts = [
            'Metacritic Best Movies of All Time',
            'Metacritic Best TV Shows of All Time',
        ]
        
        distinct_charts = db.query(
            PublicChartEntry.platform,
            PublicChartEntry.chart_name
        ).distinct().all()
        
        result = []
        processed_charts = set()
        
        for platform, chart_name in distinct_charts:
            if chart_name in metacritic_top250_charts:
                chart_key = (platform, chart_name)
                if chart_key in processed_charts:
                    continue
                processed_charts.add(chart_key)
                
                entries = db.query(PublicChartEntry).filter(
                    PublicChartEntry.platform == platform,
                    PublicChartEntry.chart_name == chart_name,
                ).order_by(PublicChartEntry.rank.asc()).all()
                
                if entries:
                    chart_entries = []
                    for e in entries:
                        poster = e.poster or ""
                        if poster and not (poster.startswith("/tmdb-images/") or poster.startswith("/api/") or poster.startswith("http")):
                            if not poster.startswith("/"):
                                poster = "/" + poster
                            poster = f"/tmdb-images{poster}"
                        
                        chart_entries.append({
                            "tmdb_id": e.tmdb_id,
                            "rank": e.rank,
                            "title": e.title,
                            "poster": poster,
                            "media_type": e.media_type,
                        })
                    
                    result.append({
                        "platform": platform,
                        "chart_name": chart_name,
                        "media_type": "both",
                        "entries": chart_entries
                    })
            else:
                distinct_media_types = db.query(PublicChartEntry.media_type).filter(
                    PublicChartEntry.platform == platform,
                    PublicChartEntry.chart_name == chart_name,
                ).distinct().all()
                
                for (media_type,) in distinct_media_types:
                    entries = db.query(PublicChartEntry).filter(
                        PublicChartEntry.platform == platform,
                        PublicChartEntry.chart_name == chart_name,
                        PublicChartEntry.media_type == media_type,
                    ).order_by(PublicChartEntry.rank.asc()).all()
                    
                    if entries:
                        chart_entries = []
                        for e in entries:
                            poster = e.poster or ""
                            if poster and not (poster.startswith("/tmdb-images/") or poster.startswith("/api/") or poster.startswith("http")):
                                if not poster.startswith("/"):
                                    poster = "/" + poster
                                poster = f"/tmdb-images{poster}"
                            
                            chart_entries.append({
                                "tmdb_id": e.tmdb_id,
                                "rank": e.rank,
                                "title": e.title,
                                "poster": poster,
                                "media_type": e.media_type,
                            })
                        
                        result.append({
                            "platform": platform,
                            "chart_name": chart_name,
                            "media_type": media_type,
                            "entries": chart_entries
                        })
        
        await set_cache(cache_key, result, expire=CHARTS_CACHE_EXPIRE)
        return result
    except Exception as e:
        logger.error(f"è·å–å…¬å¼€æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–å…¬å¼€æ¦œå•å¤±è´¥: {str(e)}")

@app.get("/api/charts/detail")
async def get_chart_detail(
    platform: str,
    chart_name: str,
    db: Session = Depends(get_db)
):
    """è·å–å®Œæ•´æ¦œå•è¯¦æƒ…ï¼ˆTop 250ï¼‰"""
    try:
        platform_map = {
            'Rotten Tomatoes': 'çƒ‚ç•ªèŒ„',
            'Metacritic': 'MTC',
        }
        backend_platform = platform_map.get(platform, platform)
        
        chart_name_map = {
            'IMDb ç”µå½± Top 250': 'IMDb Top 250 Movies',
            'IMDb å‰§é›† Top 250': 'IMDb Top 250 TV Shows',
            'Letterboxd ç”µå½± Top 250': 'Letterboxd Official Top 250',
            'è±†ç“£ ç”µå½± Top 250': 'è±†ç“£ Top 250',
            'Metacritic å²ä¸Šæœ€ä½³ç”µå½± Top 250': 'Metacritic Best Movies of All Time',
            'Metacritic å²ä¸Šæœ€ä½³å‰§é›† Top 250': 'Metacritic Best TV Shows of All Time',
            'TMDB é«˜åˆ†ç”µå½± Top 250': 'TMDB Top 250 Movies',
            'TMDB é«˜åˆ†å‰§é›† Top 250': 'TMDB Top 250 TV Shows',
        }
        backend_chart_name = chart_name_map.get(chart_name, chart_name)
        
        entries = db.query(PublicChartEntry).filter(
            PublicChartEntry.platform == backend_platform,
            PublicChartEntry.chart_name == backend_chart_name,
        ).order_by(PublicChartEntry.rank.asc()).all()
        
        if not entries:
            entries = db.query(ChartEntry).filter(
                ChartEntry.platform == backend_platform,
                ChartEntry.chart_name == backend_chart_name,
            ).order_by(ChartEntry.rank.asc()).all()
        
        if not entries:
            raise HTTPException(status_code=404, detail="æ¦œå•æ•°æ®ä¸å­˜åœ¨")
        
        chart_entries = []
        media_type = None
        
        metacritic_top250_charts = [
            'Metacritic Best Movies of All Time',
            'Metacritic Best TV Shows of All Time',
        ]
        is_metacritic_top250 = backend_chart_name in metacritic_top250_charts
        
        for e in entries:
            poster = e.poster or ""
            if poster and not (poster.startswith("/tmdb-images/") or poster.startswith("/api/") or poster.startswith("http")):
                if not poster.startswith("/"):
                    poster = "/" + poster
                poster = f"/tmdb-images{poster}"
            
            entry_media_type = getattr(e, 'media_type', None)
            if not media_type and entry_media_type:
                media_type = entry_media_type
            
            chart_entries.append({
                "tmdb_id": e.tmdb_id,
                "rank": e.rank,
                "title": e.title,
                "poster": poster,
                "media_type": entry_media_type,
            })
        
        if is_metacritic_top250:
            media_type = 'both'
        elif not media_type:
            media_type = 'movie'
        
        return {
            "platform": platform,
            "chart_name": chart_name,
            "media_type": media_type,
            "entries": chart_entries
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ¦œå•è¯¦æƒ…å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¦œå•è¯¦æƒ…å¤±è´¥: {str(e)}")

@app.post("/api/charts/auto-update")
async def auto_update_charts(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è‡ªåŠ¨æ›´æ–°æ‰€æœ‰æ¦œå•æ•°æ®"""
    require_admin(current_user)
    
    try:
        from chart_scrapers import ChartScraper
        
        scraper = ChartScraper(db)
        results = {}
        results['çƒ‚ç•ªèŒ„ç”µå½±'] = await scraper.update_rotten_movies()
        results['çƒ‚ç•ªèŒ„TV'] = await scraper.update_rotten_tv()
        results['Letterboxd'] = await scraper.update_letterboxd_popular()
        results['Metacriticç”µå½±'] = await scraper.update_metacritic_movies()
        results['Metacriticå‰§é›†'] = await scraper.update_metacritic_shows()
        results['TMDBè¶‹åŠ¿'] = await scraper.update_tmdb_trending_all_week()
        results['Traktç”µå½±'] = await scraper.update_trakt_movies_weekly()
        results['Traktå‰§é›†'] = await scraper.update_trakt_shows_weekly()
        results['IMDb'] = await scraper.update_imdb_top10()
        results['è±†ç“£ç”µå½±'] = await scraper.update_douban_weekly_movie()
        results['è±†ç“£åè¯­å‰§é›†'] = await scraper.update_douban_weekly_chinese_tv()
        results['è±†ç“£å…¨çƒå‰§é›†'] = await scraper.update_douban_weekly_global_tv()
        
        from datetime import timezone
        beijing_tz = timezone(timedelta(hours=8))
        update_time = datetime.now(beijing_tz)
        
        from chart_scrapers import scheduler_instance
        if scheduler_instance:
            scheduler_instance.last_update = update_time
            logger.info(f"æ‰‹åŠ¨æ›´æ–°åï¼Œæ›´æ–°è°ƒåº¦å™¨å®ä¾‹çš„last_update: {update_time}")
        
        try:
            db_status = db.query(SchedulerStatus).order_by(SchedulerStatus.updated_at.desc()).first()
            if db_status:
                db_status.last_update = update_time
                db.commit()
                logger.info("æ‰‹åŠ¨æ›´æ–°åï¼Œæ•°æ®åº“ä¸­çš„last_updateå·²æ›´æ–°")
        except Exception as db_error:
            logger.error(f"æ›´æ–°æ•°æ®åº“last_updateå¤±è´¥: {db_error}")
        
        return {
            "status": "success",
            "message": "æ‰€æœ‰æ¦œå•æ•°æ®å·²æˆåŠŸæ›´æ–°",
            "results": results,
            "timestamp": update_time.isoformat()
        }
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨æ›´æ–°æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨æ›´æ–°å¤±è´¥: {str(e)}")

@app.post("/api/charts/auto-update/{platform}")
async def auto_update_platform_charts(
    platform: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """è‡ªåŠ¨æ›´æ–°æŒ‡å®šå¹³å°çš„æ¦œå•æ•°æ®"""
    require_admin(current_user)
    
    try:
        from chart_scrapers import ChartScraper
        
        scraper = ChartScraper(db)
        platform_updaters = {
            "è±†ç“£": [
                scraper.update_douban_weekly_movie,
                scraper.update_douban_weekly_chinese_tv,
                scraper.update_douban_weekly_global_tv
            ],
            "IMDb": [scraper.update_imdb_top10],
            "Letterboxd": [scraper.update_letterboxd_popular],
            "çƒ‚ç•ªèŒ„": [scraper.update_rotten_movies, scraper.update_rotten_tv],
            "MTC": [scraper.update_metacritic_movies, scraper.update_metacritic_shows],
            "TMDB": [scraper.update_tmdb_trending_all_week],
            "Trakt": [scraper.update_trakt_movies_weekly, scraper.update_trakt_shows_weekly]
        }
        
        if platform not in platform_updaters:
            raise HTTPException(status_code=400, detail=f"ä¸æ”¯æŒçš„å¹³å°: {platform}")
        
        results = {}
        for i, updater in enumerate(platform_updaters[platform]):
            count = await updater()
            results[f"{platform}_{i+1}"] = count
        
        return {
            "status": "success",
            "message": f"{platform} å¹³å°æ¦œå•æ•°æ®å·²æˆåŠŸæ›´æ–°",
            "platform": platform,
            "results": results,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è‡ªåŠ¨æ›´æ–° {platform} æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è‡ªåŠ¨æ›´æ–° {platform} å¤±è´¥: {str(e)}")

@app.post("/api/charts/update-top250")
async def update_top250_chart(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ›´æ–°å•ä¸ª Top 250 æ¦œå•"""
    require_admin(current_user)
    
    try:
        body = await request.json()
        platform = body.get("platform")
        chart_name = body.get("chart_name")
        
        if not platform or not chart_name:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°ï¼šplatform å’Œ chart_name")
        
        from chart_scrapers import ChartScraper
        
        scraper = ChartScraper(db)
        top250_updaters = {
            "TMDB": {
                "TMDB Top 250 Movies": scraper.update_tmdb_top250_movies,
                "TMDB Top 250 TV Shows": scraper.update_tmdb_top250_tv,
            },
            "IMDb": {
                "IMDb Top 250 Movies": scraper.update_imdb_top250_movies,
                "IMDb Top 250 TV Shows": scraper.update_imdb_top250_tv,
            },
            "Letterboxd": {
                "Letterboxd Official Top 250": scraper.update_letterboxd_top250,
            },
            "è±†ç“£": {
                "è±†ç“£ Top 250": scraper.update_douban_top250,
            },
            "MTC": {
                "Metacritic Best Movies of All Time": scraper.update_metacritic_best_movies,
                "Metacritic Best TV Shows of All Time": scraper.update_metacritic_best_tv,
            },
            # "Trakt": {
            #     "Trakt Highest Rated Movies (Top 250)": scraper.update_trakt_top250_movies,
            #     "Trakt Highest Rated TV Shows (Top 250)": scraper.update_trakt_top250_tv,
            # },
        }
        
        if platform not in top250_updaters:
            raise HTTPException(status_code=400, detail=f"å¹³å° {platform} æš‚ä¸æ”¯æŒ Top 250 æ¦œå•æ›´æ–°")
        
        if chart_name not in top250_updaters[platform]:
            raise HTTPException(status_code=400, detail=f"å¹³å° {platform} ä¸æ”¯æŒæ¦œå•: {chart_name}")
        
        updater = top250_updaters[platform][chart_name]
        
        if platform == "è±†ç“£" and chart_name == "è±†ç“£ Top 250":
            douban_cookie = current_user.douban_cookie if current_user.douban_cookie else None
            count = await updater(douban_cookie=douban_cookie, request=request)
        else:
            count = await updater()
        
        return {
            "status": "success",
            "message": f"{platform} - {chart_name} æ›´æ–°æˆåŠŸ",
            "platform": platform,
            "chart_name": chart_name,
            "count": count,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        error_msg = str(e)
        if "ANTI_SCRAPING_DETECTED" in error_msg:
            logger.warning(f"æ›´æ–° Top 250 æ¦œå•é‡åˆ°åçˆ¬è™«æœºåˆ¶: {e}")
            raise HTTPException(
                status_code=428,
                detail={
                    "error": "ANTI_SCRAPING_DETECTED",
                    "message": "é‡åˆ°åçˆ¬è™«æœºåˆ¶ï¼Œè¯·éªŒè¯",
                    "platform": platform,
                    "chart_name": chart_name
                }
            )
        logger.error(f"æ›´æ–° Top 250 æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å¤±è´¥: {str(e)}")

@app.post("/api/charts/clear/{platform}")
async def clear_platform_charts(
    platform: str,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ¸…ç©ºæŒ‡å®šå¹³å°çš„æ‰€æœ‰æ¦œå•ï¼ˆæ’é™¤ Top 250 æ¦œå•ï¼‰"""
    require_admin(current_user)
    
    try:
        top250_chart_names = [
            "IMDb Top 250 Movies",
            "IMDb Top 250 TV Shows",
            "Letterboxd Official Top 250",
            "è±†ç“£ Top 250",
            "Metacritic Best Movies of All Time",
            "Metacritic Best TV Shows of All Time",
            "TMDB Top 250 Movies",
            "TMDB Top 250 TV Shows",
        ]
        
        deleted_count = db.query(ChartEntry).filter(
            ChartEntry.platform == platform,
            ~ChartEntry.chart_name.in_(top250_chart_names)
        ).delete()
        db.commit()
        
        return {
            "status": "success",
            "message": f"å·²æ¸…ç©º {platform} å¹³å°çš„æ‰€æœ‰æ¦œå•ï¼ˆTop 250 æ¦œå•é™¤å¤–ï¼‰ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•",
            "deleted_count": deleted_count,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"æ¸…ç©º {platform} å¹³å°æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºæ¦œå•å¤±è´¥: {str(e)}")

@app.post("/api/charts/clear-top250")
async def clear_top250_chart(
    request: Request,
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ¸…ç©ºå•ä¸ª Top 250 æ¦œå•"""
    require_admin(current_user)
    
    try:
        body = await request.json()
        platform = body.get("platform")
        chart_name = body.get("chart_name")
        
        if not platform or not chart_name:
            raise HTTPException(status_code=400, detail="ç¼ºå°‘å¿…è¦å‚æ•°ï¼šplatform å’Œ chart_name")
        
        deleted_count = db.query(ChartEntry).filter(
            ChartEntry.platform == platform,
            ChartEntry.chart_name == chart_name
        ).delete()
        db.commit()
        
        return {
            "status": "success",
            "message": f"å·²æ¸…ç©º {platform} - {chart_name}ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•",
            "platform": platform,
            "chart_name": chart_name,
            "deleted_count": deleted_count,
            "timestamp": datetime.utcnow().isoformat()
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ¸…ç©º Top 250 æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºå¤±è´¥: {str(e)}")

@app.post("/api/charts/clear-all")
async def clear_all_charts(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """æ¸…ç©ºæ‰€æœ‰å¹³å°çš„æ‰€æœ‰æ¦œå•ï¼ˆæ’é™¤ Top 250 æ¦œå•ï¼‰"""
    require_admin(current_user)
    
    try:
        top250_chart_names = [
            "IMDb Top 250 Movies",
            "IMDb Top 250 TV Shows",
            "Letterboxd Official Top 250",
            "è±†ç“£ Top 250",
            "Metacritic Best Movies of All Time",
            "Metacritic Best TV Shows of All Time",
            "TMDB Top 250 Movies",
            "TMDB Top 250 TV Shows",
        ]
        
        deleted_count = db.query(ChartEntry).filter(
            ~ChartEntry.chart_name.in_(top250_chart_names)
        ).delete()
        db.commit()
        
        return {
            "status": "success",
            "message": f"å·²æ¸…ç©ºæ‰€æœ‰å¹³å°çš„æ‰€æœ‰æ¦œå•ï¼ˆTop 250 æ¦œå•é™¤å¤–ï¼‰ï¼Œå…±åˆ é™¤ {deleted_count} æ¡è®°å½•",
            "deleted_count": deleted_count,
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"æ¸…ç©ºæ‰€æœ‰æ¦œå•å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºæ‰€æœ‰æ¦œå•å¤±è´¥: {str(e)}")

@app.post("/api/scheduler/test-notification")
async def test_notification(
    current_user: User = Depends(get_current_user)
):
    """æµ‹è¯•Telegramé€šçŸ¥"""
    require_admin(current_user)
    
    try:
        from chart_scrapers import telegram_notifier
        success = await telegram_notifier.send_message("ğŸ§ª *æµ‹è¯•é€šçŸ¥*\\n\\nè¿™æ˜¯ä¸€æ¡æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨äºéªŒè¯Telegramé€šçŸ¥åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚")
        
        if success:
            return {
                "status": "success",
                "message": "æµ‹è¯•é€šçŸ¥å‘é€æˆåŠŸ"
            }
        else:
            return {
                "status": "error",
                "message": "æµ‹è¯•é€šçŸ¥å‘é€å¤±è´¥ï¼Œè¯·æ£€æŸ¥Telegramé…ç½®"
            }
    except Exception as e:
        logger.error(f"æµ‹è¯•é€šçŸ¥å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æµ‹è¯•é€šçŸ¥å¤±è´¥: {str(e)}")

@app.get("/api/charts/status")
async def get_charts_status(db: Session = Depends(get_db)):
    """è·å–æ¦œå•æ•°æ®çŠ¶æ€"""
    try:
        platforms = ["è±†ç“£", "IMDb", "Letterboxd", "çƒ‚ç•ªèŒ„", "MTC"]
        status = {}
        
        for platform in platforms:
            latest_entries = db.query(
                ChartEntry.platform,
                ChartEntry.chart_name,
                ChartEntry.media_type,
                func.max(ChartEntry.created_at).label('latest_update')
            ).filter(
                ChartEntry.platform == platform
            ).group_by(
                ChartEntry.platform,
                ChartEntry.chart_name,
                ChartEntry.media_type
            ).all()
            
            platform_status = []
            for entry in latest_entries:
                count = db.query(ChartEntry).filter(
                    ChartEntry.platform == entry.platform,
                    ChartEntry.chart_name == entry.chart_name,
                    ChartEntry.media_type == entry.media_type
                ).count()
                
                platform_status.append({
                    "chart_name": entry.chart_name,
                    "media_type": entry.media_type,
                    "count": count,
                    "last_updated": entry.latest_update.isoformat() if entry.latest_update else None
                })
            
            status[platform] = platform_status
        
        return {
            "status": "success",
            "data": status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"è·å–æ¦œå•çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–æ¦œå•çŠ¶æ€å¤±è´¥: {str(e)}")

@app.post("/api/scheduler/start")
async def start_scheduler_endpoint(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    require_admin(current_user)
    
    try:
        from chart_scrapers import start_auto_scheduler
        logger.info(f"ç”¨æˆ· {current_user.email} å°è¯•å¯åŠ¨è°ƒåº¦å™¨")
        
        scheduler = await start_auto_scheduler(db_session=db)
        scheduler_status = scheduler.get_status()
        logger.info(f"è°ƒåº¦å™¨å¯åŠ¨æˆåŠŸï¼ŒçŠ¶æ€: {scheduler_status}")
        
        db_status = SchedulerStatus(
            running=True,
            next_update=datetime.fromisoformat(scheduler_status['next_update'].replace('+08:00', '')),
            last_update=datetime.fromisoformat(scheduler_status['last_update']) if scheduler_status['last_update'] else None
        )
        db.add(db_status)
        db.commit()
        logger.info("æ•°æ®åº“çŠ¶æ€å·²æ›´æ–°")
        
        return {
            "status": "success",
            "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²å¯åŠ¨",
            "timestamp": datetime.utcnow().isoformat(),
            "scheduler_status": scheduler_status
        }
    except Exception as e:
        logger.error(f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {e}")
        import traceback
        logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        raise HTTPException(status_code=500, detail=f"å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {str(e)}")

@app.post("/api/scheduler/stop")
async def stop_scheduler_endpoint(
    current_user: User = Depends(get_current_user),
    db: Session = Depends(get_db)
):
    """åœæ­¢å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨"""
    require_admin(current_user)
    
    try:
        from chart_scrapers import stop_auto_scheduler
        stop_auto_scheduler()
        
        db_status = SchedulerStatus(
            running=False,
            next_update=None,
            last_update=None
        )
        db.add(db_status)
        db.commit()
        logger.info("è°ƒåº¦å™¨å·²åœæ­¢ï¼Œæ•°æ®åº“çŠ¶æ€å·²æ›´æ–°")
        
        return {
            "status": "success",
            "message": "å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨å·²åœæ­¢",
            "timestamp": datetime.utcnow().isoformat()
        }
    except Exception as e:
        logger.error(f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åœæ­¢è°ƒåº¦å™¨å¤±è´¥: {str(e)}")

def calculate_next_update():
    """è®¡ç®—ä¸‹æ¬¡æ›´æ–°æ—¶é—´ï¼ˆæ¯å¤©åŒ—äº¬æ—¶é—´21:30ï¼‰"""
    from datetime import timezone, timedelta
    beijing_tz = timezone(timedelta(hours=8))
    now_beijing = datetime.now(beijing_tz)
    today_2130 = now_beijing.replace(hour=21, minute=30, second=0, microsecond=0)
    
    if now_beijing >= today_2130:
        next_update = today_2130 + timedelta(days=1)
    else:
        next_update = today_2130
    
    return next_update

@app.get("/api/scheduler/status")
async def get_scheduler_status_endpoint(db: Session = Depends(get_db)):
    """è·å–è°ƒåº¦å™¨çŠ¶æ€"""
    try:
        from chart_scrapers import scheduler_instance
        if scheduler_instance and scheduler_instance.running:
            status = scheduler_instance.get_status()
            logger.debug(f"ä»å†…å­˜è°ƒåº¦å™¨å®ä¾‹è·å–çŠ¶æ€: {status}")
            return {
                "status": "success",
                "data": status,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        db_status = db.query(SchedulerStatus).order_by(SchedulerStatus.updated_at.desc()).first()
        
        if db_status:
            logger.debug(f"ä»æ•°æ®åº“è·å–è°ƒåº¦å™¨çŠ¶æ€: running={db_status.running}")
            next_update = calculate_next_update()
            return {
                "status": "success",
                "data": {
                    "running": db_status.running,
                    "next_update": next_update.isoformat(),
                    "last_update": db_status.last_update.isoformat() if db_status.last_update else None
                },
                "timestamp": datetime.utcnow().isoformat()
            }
        else:
            from chart_scrapers import get_scheduler_status
            status = get_scheduler_status()
            logger.debug(f"ä»å†…å­˜è·å–è°ƒåº¦å™¨çŠ¶æ€: {status}")
            return {
                "status": "success",
                "data": status,
                "timestamp": datetime.utcnow().isoformat()
            }
    except Exception as e:
        logger.error(f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–è°ƒåº¦å™¨çŠ¶æ€å¤±è´¥: {str(e)}")


@app.get("/api/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    redis_status = "healthy" if redis else "unhealthy"
    
    browser_pool_stats = browser_pool.get_stats()
    browser_pool_status = "healthy" if browser_pool.initialized else "unhealthy"
    
    return {
        "status": "ok",
        "redis": redis_status,
        "browser_pool": browser_pool_status,
        "browser_pool_stats": browser_pool_stats
    }

# ==========================================
# 7. åº”ç”¨å¯åŠ¨å’Œå…³é—­äº‹ä»¶
# ==========================================

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨æ—¶åˆå§‹åŒ–"""
    global redis
    try:
        redis = await aioredis.from_url(
            REDIS_URL,
            encoding='utf-8',
            decode_responses=True
        )
        logger.info("Redisè¿æ¥æˆåŠŸ")
    except Exception as e:
        logger.error(f"Redis è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
        redis = None
    
    try:
        BROWSER_POOL_SIZE = int(os.getenv("BROWSER_POOL_SIZE", "5"))
        BROWSER_POOL_CONTEXTS = int(os.getenv("BROWSER_POOL_CONTEXTS", "3"))
        BROWSER_POOL_PAGES = int(os.getenv("BROWSER_POOL_PAGES", "5"))
        
        browser_pool.max_browsers = BROWSER_POOL_SIZE
        browser_pool.max_contexts_per_browser = BROWSER_POOL_CONTEXTS
        browser_pool.max_pages_per_context = BROWSER_POOL_PAGES
        
        await browser_pool.initialize()
        logger.info(f"æµè§ˆå™¨æ± åˆå§‹åŒ–æˆåŠŸï¼Œå…± {BROWSER_POOL_SIZE} ä¸ªæµè§ˆå™¨å®ä¾‹")
    except Exception as e:
        logger.error(f"æµè§ˆå™¨æ± åˆå§‹åŒ–å¤±è´¥: {e}")
    
    if os.getenv("ENV") != "development":
        try:
            from chart_scrapers import start_auto_scheduler
            await start_auto_scheduler()
            logger.info("ç”Ÿäº§ç¯å¢ƒï¼šå®šæ—¶è°ƒåº¦å™¨å·²è‡ªåŠ¨å¯åŠ¨")
        except Exception as e:
            logger.error(f"ç”Ÿäº§ç¯å¢ƒï¼šè‡ªåŠ¨å¯åŠ¨è°ƒåº¦å™¨å¤±è´¥: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†èµ„æº"""
    try:
        await browser_pool.cleanup()
        print("æµè§ˆå™¨æ± å·²æ¸…ç†")
    except Exception as e:
        print(f"æµè§ˆå™¨æ± æ¸…ç†å¤±è´¥: {e}")
    
    global _tmdb_client
    if _tmdb_client and not _tmdb_client.is_closed:
        try:
            await _tmdb_client.aclose()
            print("TMDB å®¢æˆ·ç«¯è¿æ¥æ± å·²å…³é—­")
        except Exception as e:
            print(f"TMDB å®¢æˆ·ç«¯æ¸…ç†å¤±è´¥: {e}")
            
